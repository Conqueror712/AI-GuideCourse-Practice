{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc9a3f44",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 基于Pytorch构建简单的三层回归神经网络\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Pytorch下采用Adam梯度下降法进行神经网络的权值更新\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m在调整模型更新权重和偏差参数的方式时，\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m不同的优化算法能使模型产生不同的效果\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m用梯度下降，随机梯度下降，还是Adam方法？\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m详见：https://zhuanlan.zhihu.com/p/27449596\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# 基于Pytorch构建简单的三层回归神经网络\n",
    "\n",
    "# Pytorch下采用Adam梯度下降法进行神经网络的权值更新\n",
    "\n",
    "'''\n",
    "在调整模型更新权重和偏差参数的方式时，\n",
    "不同的优化算法能使模型产生不同的效果\n",
    "用梯度下降，随机梯度下降，还是Adam方法？\n",
    "详见：https://zhuanlan.zhihu.com/p/27449596\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N, D_in, H, D_out = 20, 1, 64, 1\n",
    "\n",
    "# Create random input and output data\n",
    "np.random.seed(0)\n",
    "x = torch.tensor(np.arange(0, N, 1).reshape(N, D_in), dtype=torch.float32)  # 20*1\n",
    "y = x + torch.tensor(np.random.randn(N, D_out), dtype=torch.float32)  # 20*1\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use Adam; the optim package contains many other\n",
    "# optimization algorithms. The first argument to the Adam constructor tells the\n",
    "# optimizer which Tensors it should update.\n",
    "learning_rate = 1e-4  # 设置学习率\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(5000): # 通过传递x到模型中计算y的预测值\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y) # 计算和打印损失\n",
    "    if t % 200 == 0:\n",
    "        plt.cla()   # y_pred = model.predict(x)\n",
    "        plt.scatter(x.data.numpy(), y.data.numpy())\n",
    "        plt.scatter(x.data.numpy(), y_pred.data.numpy())\n",
    "        plt.plot(x.data.numpy(), y_pred.data.numpy(), 'r-', lw=1, label=\"plot figure\")\n",
    "        plt.text(0.5, 0, 't=%d:Loss=%.4f' % (t, loss), fontdict={'size': 20, 'color': 'red'})\n",
    "        plt.show()\n",
    "\n",
    "        '''\n",
    "        Before the backward pass, use the optimizer object to zero all of the\n",
    "        gradients for the variables it will update (which are the learnable\n",
    "        weights of the model). This is because by default, gradients are\n",
    "        accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "        is called. Checkout docs of torch.autograd.backward for more details.\n",
    "        '''\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 反向传播: 计算损失关于模型的梯度\n",
    "\n",
    "    # 参数\n",
    "    loss.backward()\n",
    "\n",
    "    # 在优化器上调用step函数会更新它\n",
    "\n",
    "    # 参数\n",
    "\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
